{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d3d399b-4b62-46b5-97d8-9175a8e71d71",
   "metadata": {},
   "source": [
    "# Vertical Farming\n",
    "\n",
    "## Reinforcement Learning for Farming Operations\n",
    "\n",
    "The goal of this project is to use Reinforcement Learning (RL) to automate and optimize farming operations.\n",
    "\n",
    "#### Dataset\n",
    "\n",
    "The key columns in the dataset include:\n",
    "\n",
    "- Cube ID: Likely an identifier for the sensor or location where the data was collected.\n",
    "- Timestamp: The time at which the data was recorded (e.g., 2016-01-01 00:00:01).\n",
    "- Temperature Layer A and Temperature Layer B: Temperature readings from two different layers (e.g., soil layers or greenhouse zones).\n",
    "- Humidity Layer A and Humidity Layer B: Humidity readings from two different layers.\n",
    "- Door: A binary or numeric value indicating the state of a door (e.g., open or closed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee01c520-7c31-49d9-a4c1-e28b4e482f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbdfca1-f03f-4351-a774-806dc6fe4765",
   "metadata": {},
   "source": [
    "## 1. Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73a3aa07-d3a1-48ed-a130-d57c8256c321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cube ID</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Temperature Layer A</th>\n",
       "      <th>Temperature Layer B</th>\n",
       "      <th>Door</th>\n",
       "      <th>Humidity Layer A</th>\n",
       "      <th>Humidity Layer B</th>\n",
       "      <th>Hour</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Month</th>\n",
       "      <th>Temperature Diff</th>\n",
       "      <th>Humidity Diff</th>\n",
       "      <th>Temperature Layer A Rolling Avg</th>\n",
       "      <th>Humidity Layer A Rolling Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>2016-01-01 00:00:01</td>\n",
       "      <td>21.721156</td>\n",
       "      <td>22.734969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.374497</td>\n",
       "      <td>9.404221</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.013812</td>\n",
       "      <td>-0.029724</td>\n",
       "      <td>21.721156</td>\n",
       "      <td>9.374497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95</td>\n",
       "      <td>2016-01-01 00:00:02</td>\n",
       "      <td>21.721156</td>\n",
       "      <td>25.711899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.374497</td>\n",
       "      <td>9.404221</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.990743</td>\n",
       "      <td>-0.029724</td>\n",
       "      <td>21.721156</td>\n",
       "      <td>9.374497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "      <td>2016-01-01 00:00:02</td>\n",
       "      <td>21.721156</td>\n",
       "      <td>22.734969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.374497</td>\n",
       "      <td>9.404221</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.013812</td>\n",
       "      <td>-0.029724</td>\n",
       "      <td>21.721156</td>\n",
       "      <td>9.374497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55</td>\n",
       "      <td>2016-01-01 00:00:02</td>\n",
       "      <td>21.721156</td>\n",
       "      <td>22.734969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.374497</td>\n",
       "      <td>8.594411</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.013812</td>\n",
       "      <td>0.780086</td>\n",
       "      <td>21.721156</td>\n",
       "      <td>9.374497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90</td>\n",
       "      <td>2016-01-01 00:00:03</td>\n",
       "      <td>21.721156</td>\n",
       "      <td>22.734969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.374497</td>\n",
       "      <td>9.404221</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.013812</td>\n",
       "      <td>-0.029724</td>\n",
       "      <td>21.721156</td>\n",
       "      <td>9.374497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cube ID            Timestamp  Temperature Layer A  Temperature Layer B  \\\n",
       "0       49  2016-01-01 00:00:01            21.721156            22.734969   \n",
       "1       95  2016-01-01 00:00:02            21.721156            25.711899   \n",
       "2       48  2016-01-01 00:00:02            21.721156            22.734969   \n",
       "3       55  2016-01-01 00:00:02            21.721156            22.734969   \n",
       "4       90  2016-01-01 00:00:03            21.721156            22.734969   \n",
       "\n",
       "   Door  Humidity Layer A  Humidity Layer B  Hour  DayOfWeek  Month  \\\n",
       "0   0.0          9.374497          9.404221     0          4      1   \n",
       "1   0.0          9.374497          9.404221     0          4      1   \n",
       "2   0.0          9.374497          9.404221     0          4      1   \n",
       "3   0.0          9.374497          8.594411     0          4      1   \n",
       "4   0.0          9.374497          9.404221     0          4      1   \n",
       "\n",
       "   Temperature Diff  Humidity Diff  Temperature Layer A Rolling Avg  \\\n",
       "0         -1.013812      -0.029724                        21.721156   \n",
       "1         -3.990743      -0.029724                        21.721156   \n",
       "2         -1.013812      -0.029724                        21.721156   \n",
       "3         -1.013812       0.780086                        21.721156   \n",
       "4         -1.013812      -0.029724                        21.721156   \n",
       "\n",
       "   Humidity Layer A Rolling Avg  \n",
       "0                      9.374497  \n",
       "1                      9.374497  \n",
       "2                      9.374497  \n",
       "3                      9.374497  \n",
       "4                      9.374497  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertical_farm = pd.read_csv('cleaned_cubes.csv')\n",
    "vertical_farm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51395cf6-5cd9-41bc-915a-2dd637ccf96d",
   "metadata": {},
   "source": [
    "**Reinforcement Learning (RL) requires defining states, actions, and rewards**\n",
    "\n",
    "This project uses the Proximal Policy Optimization (PPO) algorithm from the stable_baselines3 library. It provides detailed information about the training process, including performance metrics and hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78f97843-46e3-4875-b3b4-2d529ac7f160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\milto\\anaconda3\\envs\\torch_env\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 79   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 25   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 79         |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 51         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02019383 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.09      |\n",
      "|    explained_variance   | -42.9      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0489    |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.00564   |\n",
      "|    value_loss           | 0.00249    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 78          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 78          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006558303 |\n",
      "|    clip_fraction        | 0.0298      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | -9.56       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000691   |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00178    |\n",
      "|    value_loss           | 0.000388    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 77          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 105         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013281226 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | -6.28       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00286    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00969    |\n",
      "|    value_loss           | 0.000469    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 76          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 133         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009864757 |\n",
      "|    clip_fraction        | 0.0983      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | -4.17       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0128     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00503    |\n",
      "|    value_loss           | 0.000272    |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define a custom environment for farming operations\n",
    "class FarmingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(FarmingEnv, self).__init__()\n",
    "        self.data = data\n",
    "        self.current_step = 0\n",
    "\n",
    "        # Exclude non-numeric columns (e.g., 'Timestamp') from the state\n",
    "        self.state_columns = self.data.select_dtypes(include=[np.number]).columns\n",
    "        self.state = self.data[self.state_columns].iloc[self.current_step]\n",
    "\n",
    "        # Define action and observation spaces\n",
    "        self.action_space = spaces.Discrete(3)  # Example: 3 actions (e.g., water, fertilize, do nothing)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, \n",
    "            high=100, \n",
    "            shape=(len(self.state),), \n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.state = self.data[self.state_columns].iloc[self.current_step]\n",
    "        return self.state.values\n",
    "\n",
    "    def step(self, action):\n",
    "        # Simulate the effect of the action (e.g., adjust temperature, humidity)\n",
    "        reward = self.calculate_reward(action)\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.data) - 1\n",
    "        self.state = self.data[self.state_columns].iloc[self.current_step]\n",
    "        return self.state.values, reward, done, {}\n",
    "\n",
    "    def calculate_reward(self, action):\n",
    "        # Define a reward function based on farming goals\n",
    "        reward = 0\n",
    "        # Example: Reward for maintaining optimal temperature and humidity\n",
    "        if 20 <= self.state['Temperature Layer A'] <= 25 and 40 <= self.state['Humidity Layer A'] <= 60:\n",
    "            reward += 1\n",
    "        return reward\n",
    "\n",
    "# Create the environment\n",
    "env = FarmingEnv(vertical_farm)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Train a PPO model\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"farming_rl_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43cd6d5-c8a0-40fd-9f4e-fdf84572deb9",
   "metadata": {},
   "source": [
    "## Observations and Insights\n",
    "\n",
    "**Stable Training**:\n",
    "- The approx_kl values are small (e.g., 0.0098), which indicates that the policy is not changing too drastically between updates. This is a good sign of stable training.\n",
    "- The clip_fraction values are reasonable (e.g., 0.0983), meaning the PPO clipping mechanism is working as intended.\n",
    "\n",
    "**Potential Issues**:\n",
    "\n",
    "The explained_variance is negative (e.g., -4.17), which suggests the value function is not predicting returns accurately. This could be due to:\n",
    "- A poorly designed reward function.\n",
    "- Insufficient training data or episodes.\n",
    "- A mismatch between the environment dynamics and the model's assumptions.\n",
    "\n",
    "The loss values are negative, which is unusual. This could indicate issues with the reward function or environment setup.\n",
    "\n",
    "**Exploration**: \n",
    "- The entropy_loss values (e.g., -1.07) indicate moderate exploration. If the entropy loss becomes too low, the policy may stop exploring.\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "Check the Reward Function:\n",
    "- Ensure the reward function is well-designed and provides meaningful feedback to the agent.\n",
    "- Avoid sparse rewards or rewards that are too small in magnitude.\n",
    "\n",
    "Increase Training Time:\n",
    "- The model has only completed 10,240 timesteps. Consider training for more timesteps (e.g., 100,000 or more) to allow the model to learn better.\n",
    "\n",
    "Tune Hyperparameters:\n",
    "- Experiment with different values for clip_range, learning_rate, and entropy_coeff to improve performance.\n",
    "- For example, try reducing the learning_rate to 0.0001 or increasing the clip_range to 0.3.\n",
    "\n",
    "Evaluate the Environment:\n",
    "- Ensure the environment is providing meaningful observations and rewards.\n",
    "- Debug the environment to confirm it behaves as expected.\n",
    "\n",
    "Monitor Progress:\n",
    "- Continue monitoring the training logs to ensure the explained_variance improves and the loss becomes positive.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Run the training for more timesteps and observe if the metrics improve.\n",
    "\n",
    "2. If the explained_variance remains negative or the loss stays unusual, revisit the reward function and environment design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45944f5e-04b4-41b1-b969-4f4201b38aad",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This project has the potential to significantly improve farming efficiency and sustainability by automating decision-making and optimizing resource usage. By reducing waste and maximizing crop yield, such systems can contribute to more sustainable agricultural practices, addressing global challenges like food security and resource scarcity.\n",
    "\n",
    "In conclusion, this project serves as a strong foundation for applying AI and machine learning to agriculture, showcasing the transformative potential of these technologies in real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c709cf-4db6-4fc4-bf43-58bdd8d4b1de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch_env)",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
